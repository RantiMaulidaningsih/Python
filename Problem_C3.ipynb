{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RantiMaulidaningsih/Python/blob/main/Problem_C3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rqlWO7tF8L2O",
        "outputId": "3ef1b0c7-8002-4d98-a2c9-7608d6a42bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.8.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (23.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.32.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (4.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4.3)\n",
            "Collecting protobuf>=3.9.2 (from tensorflow==2.9.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.5.26\n",
            "    Uninstalling flatbuffers-23.5.26:\n",
            "      Successfully uninstalled flatbuffers-23.5.26\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.1\n",
            "    Uninstalling tensorboard-data-server-0.7.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-datasets==4.6.0\n",
            "  Downloading tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (1.4.0)\n",
            "Collecting dill (from tensorflow-datasets==4.6.0)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (1.22.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (3.19.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets==4.6.0) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.6.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.6.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.6.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.6.0) (3.4)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->tensorflow-datasets==4.6.0) (5.12.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->tensorflow-datasets==4.6.0) (4.6.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->tensorflow-datasets==4.6.0) (3.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets==4.6.0) (1.59.1)\n",
            "Collecting protobuf>=3.12.2 (from tensorflow-datasets==4.6.0)\n",
            "  Downloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, dill, tensorflow-datasets\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.9.2\n",
            "    Uninstalling tensorflow-datasets-4.9.2:\n",
            "      Successfully uninstalled tensorflow-datasets-4.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.6 protobuf-4.23.3 tensorflow-datasets-4.6.0\n",
            "Collecting pillow==9.1.1\n",
            "  Downloading Pillow-9.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed pillow-9.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.4.2\n",
            "  Downloading pandas-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.2) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.2) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.4.2) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.4.2\n",
            "Requirement already satisfied: numpy==1.22.4 in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Collecting scipy==1.7.3\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from scipy==1.7.3) (1.22.4)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.7.3\n",
            "Collecting protobuf==3.20.0\n",
            "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.23.3\n",
            "    Uninstalling protobuf-4.23.3:\n",
            "      Successfully uninstalled protobuf-4.23.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.20.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-functions 1.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow==2.9.0\n",
        "!pip install tensorflow-datasets==4.6.0\n",
        "!pip install pillow==9.1.1\n",
        "!pip install pandas==1.4.2\n",
        "!pip install numpy==1.22.4\n",
        "!pip install scipy==1.7.3\n",
        "!pip install protobuf==3.20.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V0LzNTQIpC4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-QHie4Y9i48",
        "outputId": "b9b1bddf-c2bd-458a-cfa8-7f98f1e68840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.10   2         auto mode\n",
            "  1            /usr/bin/python3.10   2         manual mode\n",
            "  2            /usr/bin/python3.8    1         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in manual mode\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.9 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.9 [231 kB]\n",
            "Fetched 2,389 kB in 0s (5,367 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 123069 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.9_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.9) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.9_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.9) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.9) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ],
      "source": [
        "#restart runtime\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "!sudo update-alternatives --config python3\n",
        "!sudo apt install python3-pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wz_YgzCd-4_y",
        "outputId": "f085c2f5-3ff8-4d9d-c25f-124404b67eca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_nDJ8YT9rck",
        "outputId": "ce98c5c0-5d4b-46f1-ee89-2e51c80657a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVnlrnPn9BXD"
      },
      "source": [
        "***\n",
        "###PROBLEM B\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_DZD08C_NnT",
        "outputId": "750596f0-0c78-4434-9f31-65f4cebac5ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.3705 - accuracy: 0.8669\n",
            "Accuracy more than 83%, stop train!\n",
            "1875/1875 [==============================] - 19s 5ms/step - loss: 0.3703 - accuracy: 0.8669 - val_loss: 0.3446 - val_accuracy: 0.8755\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# PROBLEM B2\n",
        "#\n",
        "# Build a classifier for the Fashion MNIST dataset.\n",
        "# The test will expect it to classify 10 classes.\n",
        "# The input shape should be 28x28 monochrome. Do not resize the data.\n",
        "# Your input layer should accept (28, 28) as the input shape.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# Desired accuracy AND validation_accuracy > 83%\n",
        "# =============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def solution_B2():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # NORMALIZE YOUR IMAGE HERE\n",
        "    (train_pictures, train_labels), (test_pictures, test_labels) = fashion_mnist.load_data()\n",
        "    train_pictures = train_pictures.reshape(*train_pictures.shape, 1)\n",
        "    test_pictures = test_pictures.reshape(*test_pictures.shape, 1)\n",
        "    train_pictures, test_pictures = train_pictures / 255.0 , test_pictures / 255\n",
        "    train_labels, test_labels = tf.keras.utils.to_categorical(train_labels) , tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "\n",
        "    # DEFINE YOUR MODEL HERE\n",
        "    # End with 10 Neuron Dense, activated by softmax\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation=tf.nn.relu, input_shape=(28,28,1)),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)\n",
        "        ])\n",
        "    # COMPILE MODEL HERE\n",
        "    class b2Callback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy')>0.84 and logs.get('val_accuracy')>0.84):\n",
        "            print(\"\\nAccuracy more than 83%, stop train!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "    model.compile(loss= 'categorical_crossentropy',\n",
        "                  optimizer='RMSprop',  metrics=['accuracy'])\n",
        "    # TRAIN YOUR MODEL HERE\n",
        "    history = model.fit(train_pictures,\n",
        "                        train_labels,\n",
        "                        epochs=10,\n",
        "                        validation_data=(test_pictures, test_labels),\n",
        "                        callbacks=[b2Callback()\n",
        "                        ])\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_B2()\n",
        "    model.save(\"model_B2.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29jT6qXH_W06",
        "outputId": "745aa62f-4184-450c-9cd4-686657091fb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # NORMALIZE YOUR IMAGE HERE\n",
        "    (train_pictures, train_labels), (test_pictures, test_labels) = fashion_mnist.load_data()\n",
        "    train_pictures = train_pictures.reshape(*train_pictures.shape, 1)\n",
        "    test_pictures = test_pictures.reshape(*test_pictures.shape, 1)\n",
        "    train_pictures, test_pictures = train_pictures / 255.0 , test_pictures / 255\n",
        "    print(train_pictures.shape , test_pictures.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NILqSB_VZIxS"
      },
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# PROBLEM B3\n",
        "#\n",
        "# Build a CNN based classifier for Rock-Paper-Scissors dataset.\n",
        "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
        "# This is unlabeled data, use ImageDataGenerator to automatically label it.\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "#\n",
        "# Desired accuracy AND validation_accuracy > 83%\n",
        "# ========================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def solution_B3():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/releases/download/release-rps/rps.zip'\n",
        "    urllib.request.urlretrieve(data_url, 'rps.zip')\n",
        "    local_file = 'rps.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = \"data/rps/\"\n",
        "    training_datagen = ImageDataGenerator(\n",
        "        rescale=1 / 255.0,\n",
        "        rotation_range=40,\n",
        "        height_shift_range=0.2,\n",
        "        width_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.3)\n",
        "        # YOUR CODE HERE)\n",
        "    validation_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
        "    # Make sure you used \"categorical\"\n",
        "    train_generator=training_datagen.flow_from_directory(\n",
        "        TRAINING_DIR,\n",
        "        target_size=(150,150),\n",
        "        batch_size = 4,\n",
        "        subset='training',\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical') # YOUR CODE HERE\n",
        "\n",
        "    valid_generator=training_datagen.flow_from_directory(\n",
        "        TRAINING_DIR,\n",
        "        target_size=(150,150),\n",
        "        subset='validation',\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical')\n",
        "\n",
        "    model=tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # COMPILE MODEL HERE\n",
        "    class b3Callback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy')>0.84 and logs.get('val_accuracy')>0.84):\n",
        "            print(\"\\nAccuracy more than 83%, stop train!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "    model.compile(loss= 'categorical_crossentropy',\n",
        "                  optimizer='adam',  metrics=['accuracy'])\n",
        "    # TRAIN YOUR MODEL HERE\n",
        "    history = model.fit(train_generator,\n",
        "                        epochs=50,\n",
        "                        steps_per_epoch = 100,\n",
        "                        validation_data=valid_generator,\n",
        "                        validation_steps = 5,\n",
        "                        callbacks=[b3Callback()])\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_B3()\n",
        "    model.save(\"model_B3.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STa-OYZBwYAD"
      },
      "outputs": [],
      "source": [
        "# ===================================================================================================\n",
        "# PROBLEM B4\n",
        "#\n",
        "# Build and train a classifier for the BBC-text dataset.\n",
        "# This is a multiclass classification problem.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in: http://mlg.ucd.ie/datasets/bbc.html.\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 91%\n",
        "# ===================================================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def solution_B4():\n",
        "    bbc = pd.read_csv('https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/bbc-text.csv')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Make sure you used all of these parameters or you can not pass this test\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    padding_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_portion = .8\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Using \"shuffle=False\"\n",
        "    labels = bbc['category'].tolist()\n",
        "    sentences = bbc['text'].tolist()\n",
        "\n",
        "    training_sentences, validation_sentences = train_test_split(sentences, test_size=1-training_portion, shuffle =  False)  # YOUR CODE HERE\n",
        "    training_labels, validation_labels = train_test_split(labels, test_size=1-training_portion, shuffle =  False) #YOUR CODE HERE\n",
        "\n",
        "    # Fit your tokenizer with training data\n",
        "    tokenizer =  Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "    word_index = tokenizer.word_index# YOUR CODE HERE\n",
        "\n",
        "    trainSeq = tokenizer.texts_to_sequences(training_sentences)\n",
        "    trainPad_Sequences = pad_sequences(trainSeq, padding=padding_type, maxlen=max_length, truncating=trunc_type)\n",
        "    validSeq = tokenizer.texts_to_sequences(validation_sentences)\n",
        "    validPad_sequences = pad_sequences(validSeq, padding=padding_type, maxlen=max_length, truncating=trunc_type)\n",
        "    # You can also use Tokenizer to encode your label.\n",
        "    labelTokenizer = Tokenizer()\n",
        "    labelTokenizer.fit_on_texts(labels)\n",
        "    labelWord_index = labelTokenizer.word_index\n",
        "    train_labelSeq = labelTokenizer.texts_to_sequences(training_labels)\n",
        "    train_labelSeq = np.array(train_labelSeq)\n",
        "    valid_label_Seq = labelTokenizer.texts_to_sequences(validation_labels)\n",
        "    valid_label_Seq = np.array(valid_label_Seq)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        # YOUR CODE HERE.\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(units=8, activation = tf.nn.relu),\n",
        "        # YOUR CODE HERE. DO not change the last layer or test may fail\n",
        "        tf.keras.layers.Dense(6, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.summary\n",
        "    # Make sure you are using \"sparse_categorical_crossentropy\" as a loss fuction\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    class b4Callback(tf.keras.callbacks.Callback):\n",
        "        acc_goal_accepted = 0\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if (logs.get('accuracy') > 0.92 and logs.get('val_accuracy') > 0.92):\n",
        "                acc_goal_accepted += 1\n",
        "                if acc_goal_accepted >  5:\n",
        "                    self.model.stop_training = True\n",
        "\n",
        "    model.fit(\n",
        "        trainPad_Sequences,\n",
        "        train_labelSeq,\n",
        "        epochs=100,\n",
        "        validation_data=(\n",
        "            validPad_sequences,\n",
        "            valid_label_Seq),\n",
        "        verbose=2,\n",
        "        callbacks = [b4Callback()])\n",
        "\n",
        "    return model\n",
        "\n",
        "    # The code below is to save your model as a .h5 file.\n",
        "    # It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_B4()\n",
        "    model.save(\"model_B4.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0YzKkFLCQK2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================================\n",
        "# PROBLEM B5\n",
        "#\n",
        "# Build and train a neural network model using the Daily Max Temperature.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from https://github.com/jbrownlee/Datasets\n",
        "#\n",
        "# Desired MAE < 0.2 on the normalized dataset.\n",
        "# ============================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import urllib\n",
        "\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_B5():\n",
        "    data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-max-temperatures.csv'\n",
        "    urllib.request.urlretrieve(data_url, 'daily-max-temperatures.csv')\n",
        "\n",
        "    time_step = []\n",
        "    temps = []\n",
        "\n",
        "    with open('daily-max-temperatures.csv') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        step = 0\n",
        "        for row in reader:\n",
        "            temps.append(float(row[1])) # YOUR CODE HERE)\n",
        "            time_step.append(row[0]) # YOUR CODE HERE)\n",
        "            step=step + 1\n",
        "\n",
        "    series= temps # YOUR CODE HERE\n",
        "\n",
        "    # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "    min=np.min(series)\n",
        "    max=np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time=np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time=2500\n",
        "\n",
        "    time_train=time[:split_time]# YOUR CODE HERE\n",
        "    x_train=series[:split_time]# YOUR CODE HERE\n",
        "    time_valid=time[split_time:]# YOUR CODE HERE\n",
        "    x_valid=series[split_time:]# YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size=64\n",
        "    batch_size=256\n",
        "    shuffle_buffer_size=1000\n",
        "\n",
        "    train_set=windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "    print(train_set)\n",
        "    print(x_train.shape)\n",
        "\n",
        "    model=tf.keras.models.Sequential([\n",
        "        # YOUR CODE HERE.\n",
        "        tf.keras.layers.Conv1D(filters=64, kernel_size=5,\n",
        "                               strides=1, padding=\"causal\",\n",
        "                               activation=\"relu\", input_shape=[window_size, 1]),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8)),\n",
        "        tf.keras.layers.Dense(60, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "\n",
        "    class B5endEpoch(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if logs[\"mean_absolute_error\"] < 0.2 :\n",
        "                print('\\nmae less than 0.2, stop training!')\n",
        "                self.model.stop_training = True\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3, momentum = 0.9),\n",
        "                  metrics=[\"mean_absolute_error\"])\n",
        "\n",
        "    model.fit(train_set, epochs=50, verbose=1, callbacks=[B5endEpoch()])\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_B5()\n",
        "    model.save(\"model_B5.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LoG-e9j8Ta0"
      },
      "source": [
        "***\n",
        "###PROBLEM C\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTy8BuoS8fDA",
        "outputId": "58b929ae-e141-4ef7-ea24-1c802dfd6181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 18.6702 - val_loss: 9.2931\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 2.8009 - val_loss: 0.9049\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7039 - val_loss: 0.4099\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3145 - val_loss: 0.7235\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2401 - val_loss: 0.9013\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2192 - val_loss: 0.9511\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2071 - val_loss: 0.9421\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1967 - val_loss: 0.9106\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.1871 - val_loss: 0.8717\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1779 - val_loss: 0.8313\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.1692 - val_loss: 0.7916\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.1608 - val_loss: 0.7533\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.1529 - val_loss: 0.7167\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1454 - val_loss: 0.6817\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1382 - val_loss: 0.6484\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.1314 - val_loss: 0.6166\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1249 - val_loss: 0.5864\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.1187 - val_loss: 0.5576\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1128 - val_loss: 0.5301\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1072 - val_loss: 0.5040\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1019 - val_loss: 0.4791\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0968 - val_loss: 0.4555\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0920 - val_loss: 0.4329\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0874 - val_loss: 0.4114\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0831 - val_loss: 0.3910\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0789 - val_loss: 0.3716\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0750 - val_loss: 0.3531\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0712 - val_loss: 0.3355\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0676 - val_loss: 0.3187\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0642 - val_loss: 0.3027\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0610 - val_loss: 0.2876\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0579 - val_loss: 0.2731\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0550 - val_loss: 0.2594\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0522 - val_loss: 0.2464\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0496 - val_loss: 0.2340\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0471 - val_loss: 0.2221\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0447 - val_loss: 0.2109\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0424 - val_loss: 0.2002\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0403 - val_loss: 0.1901\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0382 - val_loss: 0.1805\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0363 - val_loss: 0.1713\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0344 - val_loss: 0.1626\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0326 - val_loss: 0.1543\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0310 - val_loss: 0.1464\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0294 - val_loss: 0.1389\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0279 - val_loss: 0.1318\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0265 - val_loss: 0.1251\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0251 - val_loss: 0.1187\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0238 - val_loss: 0.1126\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0226 - val_loss: 0.1068\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0214 - val_loss: 0.1013\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0203 - val_loss: 0.0961\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0193 - val_loss: 0.0911\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0183 - val_loss: 0.0864\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0173 - val_loss: 0.0820\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0164 - val_loss: 0.0777\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0156 - val_loss: 0.0737\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0148 - val_loss: 0.0699\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0140 - val_loss: 0.0663\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0133 - val_loss: 0.0628\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0126 - val_loss: 0.0596\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0119 - val_loss: 0.0565\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0113 - val_loss: 0.0535\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0107 - val_loss: 0.0508\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0102 - val_loss: 0.0481\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0096 - val_loss: 0.0456\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0091 - val_loss: 0.0432\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0086 - val_loss: 0.0410\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0082 - val_loss: 0.0388\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0078 - val_loss: 0.0368\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0074 - val_loss: 0.0349\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0070 - val_loss: 0.0330\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0066 - val_loss: 0.0313\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0063 - val_loss: 0.0297\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0059 - val_loss: 0.0281\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0056 - val_loss: 0.0266\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0053 - val_loss: 0.0252\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0050 - val_loss: 0.0239\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0048 - val_loss: 0.0226\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0045 - val_loss: 0.0214\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0043 - val_loss: 0.0203\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0041 - val_loss: 0.0192\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0038 - val_loss: 0.0182\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0036 - val_loss: 0.0173\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0034 - val_loss: 0.0164\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0033 - val_loss: 0.0155\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0031 - val_loss: 0.0147\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0029 - val_loss: 0.0139\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0028 - val_loss: 0.0132\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0026 - val_loss: 0.0125\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0025 - val_loss: 0.0118\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0024 - val_loss: 0.0112\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0022 - val_loss: 0.0106\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0021 - val_loss: 0.0100\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0020 - val_loss: 0.0095\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0019 - val_loss: 0.0090\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0018 - val_loss: 0.0085\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0015 - val_loss: 0.0072\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0014 - val_loss: 0.0068\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0014 - val_loss: 0.0065\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0013 - val_loss: 0.0061\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0012 - val_loss: 0.0058\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 9.8292e-04 - val_loss: 0.0047\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 9.3051e-04 - val_loss: 0.0044\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 8.8087e-04 - val_loss: 0.0042\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 8.3387e-04 - val_loss: 0.0040\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 7.8937e-04 - val_loss: 0.0037\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 7.4723e-04 - val_loss: 0.0035\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 7.0733e-04 - val_loss: 0.0034\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 6.6956e-04 - val_loss: 0.0032\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 6.3379e-04 - val_loss: 0.0030\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 5.9993e-04 - val_loss: 0.0028\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 5.6786e-04 - val_loss: 0.0027\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 5.3751e-04 - val_loss: 0.0026\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 5.0877e-04 - val_loss: 0.0024\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 4.8156e-04 - val_loss: 0.0023\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 4.5581e-04 - val_loss: 0.0022\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 4.3143e-04 - val_loss: 0.0020\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 4.0834e-04 - val_loss: 0.0019\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 3.8649e-04 - val_loss: 0.0018\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 3.6580e-04 - val_loss: 0.0017\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 3.4622e-04 - val_loss: 0.0016\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.2768e-04 - val_loss: 0.0016\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 3.1013e-04 - val_loss: 0.0015\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 2.9352e-04 - val_loss: 0.0014\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.7780e-04 - val_loss: 0.0013\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.6292e-04 - val_loss: 0.0012\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.4883e-04 - val_loss: 0.0012\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.3549e-04 - val_loss: 0.0011\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.2287e-04 - val_loss: 0.0011\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.1093e-04 - val_loss: 0.0010\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.9962e-04 - val_loss: 9.4766e-04\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.8891e-04 - val_loss: 8.9689e-04\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7878e-04 - val_loss: 8.4879e-04\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6919e-04 - val_loss: 8.0329e-04\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.6012e-04 - val_loss: 7.6023e-04\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5153e-04 - val_loss: 7.1942e-04\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.4340e-04 - val_loss: 6.8087e-04\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3570e-04 - val_loss: 6.4434e-04\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2842e-04 - val_loss: 6.0973e-04\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.2153e-04 - val_loss: 5.7704e-04\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1501e-04 - val_loss: 5.4605e-04\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0883e-04 - val_loss: 5.1675e-04\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0299e-04 - val_loss: 4.8903e-04\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 9.7462e-05 - val_loss: 4.6276e-04\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 9.2229e-05 - val_loss: 4.3791e-04\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 8.7277e-05 - val_loss: 4.1442e-04\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 8.2590e-05 - val_loss: 3.9215e-04\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 7.8153e-05 - val_loss: 3.7110e-04\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 7.3955e-05 - val_loss: 3.5116e-04\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 6.9983e-05 - val_loss: 3.3229e-04\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 6.6224e-05 - val_loss: 3.1445e-04\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 6.2666e-05 - val_loss: 2.9754e-04\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 5.9299e-05 - val_loss: 2.8158e-04\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 5.6113e-05 - val_loss: 2.6644e-04\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 5.3098e-05 - val_loss: 2.5212e-04\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 5.0245e-05 - val_loss: 2.3858e-04\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 4.7544e-05 - val_loss: 2.2575e-04\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 4.4990e-05 - val_loss: 2.1363e-04\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 4.2571e-05 - val_loss: 2.0216e-04\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 4.0283e-05 - val_loss: 1.9128e-04\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 3.8117e-05 - val_loss: 1.8100e-04\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 3.6069e-05 - val_loss: 1.7127e-04\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 3.4129e-05 - val_loss: 1.6205e-04\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 3.2294e-05 - val_loss: 1.5335e-04\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 3.0558e-05 - val_loss: 1.4511e-04\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 2.8915e-05 - val_loss: 1.3730e-04\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 2.7360e-05 - val_loss: 1.2992e-04\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 2.5889e-05 - val_loss: 1.2294e-04\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.4497e-05 - val_loss: 1.1633e-04\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.3179e-05 - val_loss: 1.1006e-04\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 2.1933e-05 - val_loss: 1.0415e-04\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0753e-05 - val_loss: 9.8547e-05\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.9637e-05 - val_loss: 9.3246e-05\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8581e-05 - val_loss: 8.8227e-05\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7581e-05 - val_loss: 8.3487e-05\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6635e-05 - val_loss: 7.9000e-05\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5740e-05 - val_loss: 7.4756e-05\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.4894e-05 - val_loss: 7.0721e-05\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.4092e-05 - val_loss: 6.6921e-05\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.3334e-05 - val_loss: 6.3314e-05\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.2617e-05 - val_loss: 5.9920e-05\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1938e-05\n",
            "Target has acc, stop processing!!!\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.1938e-05 - val_loss: 5.6682e-05\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "[[-0.00725338]\n",
            " [ 6.0077944 ]]\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# PROBLEM C1\n",
        "#\n",
        "# Given two arrays, train a neural network model to match the X to the Y.\n",
        "# Predict the model with new values of X [-2.0, 10.0]\n",
        "# We provide the model prediction, do not change the code.\n",
        "#\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1]\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# Please be aware that this is a linear model.\n",
        "# We will test your model with values in a range as defined in the array to make sure your model is linear.\n",
        "#\n",
        "# Desired loss (MSE) < 1e-4\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def solution_C1():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    X = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)\n",
        "    Y = np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    def make_linear_model():\n",
        "        linear_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1), dtype=float),\n",
        "        tf.keras.layers.Dense(16, activation=\"linear\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"linear\")])\n",
        "        linear_model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\n",
        "        return linear_model\n",
        "    model = make_linear_model()\n",
        "\n",
        "    #computing with human\n",
        "    true_value  = [0.0, 6.0]\n",
        "    prediction_value = [-2.0, 10.0]\n",
        "\n",
        "    class c1callback(tf.keras.callbacks.Callback):\n",
        "      target_met = 0\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('loss') < 1e-4):\n",
        "          self.target_met +=1\n",
        "          if self.target_met > 10:\n",
        "            print(\"\\nTarget has acc, stop processing!!!\")\n",
        "            self.model.stop_training = True\n",
        "          self.target_met\n",
        "\n",
        "    history = model.fit(X, Y, epochs= 500,\n",
        "                        callbacks =[c1callback()])\n",
        "    print(model.predict([-2.0, 10.0]))\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_C1()\n",
        "    model.save(\"model_C1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKz4yjamcuEM",
        "outputId": "14a4899a-e703-497d-ae7b-be57ec9770da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2009 - acc: 0.9400 - val_loss: 0.1023 - val_acc: 0.9698\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0881 - acc: 0.9744 - val_loss: 0.0835 - val_acc: 0.9749\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0653 - acc: 0.9816 - val_loss: 0.1014 - val_acc: 0.9735\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0505 - acc: 0.9857 - val_loss: 0.0880 - val_acc: 0.9785\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0405 - acc: 0.9887 - val_loss: 0.0841 - val_acc: 0.9799\n",
            "Epoch 6/50\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9913\n",
            "Accuracy and val_accuracy more than 91%, stop processing!!!\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0343 - acc: 0.9913 - val_loss: 0.0907 - val_acc: 0.9798\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# PROBLEM C2\n",
        "#\n",
        "# Create a classifier for the MNIST Handwritten digit dataset.\n",
        "# The test will expect it to classify 10 classes.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# Desired accuracy AND validation_accuracy > 91%\n",
        "# =============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def solution_C2():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "\n",
        "    # NORMALIZE YOUR IMAGE HERE\n",
        "    (train_pictures, train_labels), (test_pictures, test_labels) = mnist.load_data()\n",
        "    train_pictures = train_pictures / 255\n",
        "    test_pictures = test_pictures / 255\n",
        "\n",
        "    # DEFINE YOUR MODEL HERE\n",
        "    # End with 10 Neuron Dense, activated by softmax\n",
        "    def c2_model():\n",
        "      classifier_model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Input(shape=(28,28,1)),\n",
        "          tf.keras.layers.Flatten(),\n",
        "          tf.keras.layers.Dense(units= 512, activation = tf.nn.relu),\n",
        "          tf.keras.layers.Dense(10, activation = 'softmax')])\n",
        "      return classifier_model\n",
        "\n",
        "    model = c2_model()\n",
        "    # COMPILE MODEL HERE\n",
        "    model.compile(loss= 'sparse_categorical_crossentropy', optimizer='RMSprop', metrics= ['acc'])\n",
        "    # TRAIN YOUR MODEL HERE\n",
        "\n",
        "    class c2callback(tf.keras.callbacks.Callback):\n",
        "      target_met = 0\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('acc') > 0.92)and (logs.get('val_acc') > 0.92):\n",
        "          self.target_met +=1\n",
        "          if self.target_met > 5:\n",
        "            print(\"\\nAccuracy and val_accuracy more than 91%, stop processing!!!\")\n",
        "            self.model.stop_training = True\n",
        "          self.target_met\n",
        "\n",
        "    history = model.fit(train_pictures, train_labels,\n",
        "                        epochs= 50, validation_data=(test_pictures, test_labels),\n",
        "                        callbacks =[c2callback()])\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_C2()\n",
        "    model.save(\"model_C2.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLTaP7MxpYEj",
        "outputId": "142d8843-5c5e-4cd5-e3b1-8b64a62451e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/40\n",
            "40/40 [==============================] - 112s 3s/step - loss: 4.7502 - accuracy: 0.4437 - val_loss: 0.6662 - val_accuracy: 0.5125\n",
            "Epoch 2/40\n",
            "40/40 [==============================] - 106s 3s/step - loss: 0.7384 - accuracy: 0.5250 - val_loss: 0.6848 - val_accuracy: 0.6000\n",
            "Epoch 3/40\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.7630 - accuracy: 0.5688 - val_loss: 0.6897 - val_accuracy: 0.5750\n",
            "Epoch 4/40\n"
          ]
        }
      ],
      "source": [
        "# =======================================================================================================\n",
        "# PROBLEM C3\n",
        "#\n",
        "# Build a CNN based classifier for Cats vs Dogs dataset.\n",
        "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
        "# This is unlabeled data, use ImageDataGenerator to automatically label it.\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in https://www.kaggle.com/c/dogs-vs-cats/data\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 72%\n",
        "# ========================================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def solution_C3():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/cats_and_dogs.zip'\n",
        "    urllib.request.urlretrieve(data_url, 'cats_and_dogs.zip')\n",
        "    local_file = 'cats_and_dogs.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    BASE_DIR = 'data/cats_and_dogs_filtered'\n",
        "    train_dir = os.path.join(BASE_DIR, 'train')\n",
        "    validation_dir = os.path.join(BASE_DIR, 'validation')\n",
        "\n",
        "    train_datagen =  ImageDataGenerator(\n",
        "        rescale=1 / 255.0)\n",
        "\n",
        "    valid_datagen = ImageDataGenerator(rescale = 1/255.0) # YOUR CODE HERE\n",
        "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
        "    # Make sure you used \"binary\"\n",
        "    train_generator =  train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150,150),\n",
        "        batch_size = 4,\n",
        "        color_mode='rgb',\n",
        "        class_mode='binary')\n",
        "\n",
        "    valid_generator =valid_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150,150),\n",
        "        batch_size = 4,\n",
        "        color_mode='rgb',\n",
        "        class_mode='binary')# YOUR CODE HERE\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # YOUR CODE HERE, end with a Neuron Dense, activated by 'sigmoid'\n",
        "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    class C3Callback(tf.keras.callbacks.Callback):\n",
        "      target_holded = 0\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "          if (logs.get('accuracy') > 0.72 and logs.get('val_accuracy') > 0.72):\n",
        "             print(\"\\nAccuracy more than 72%, stop train!\")\n",
        "             self.target_holded +=1\n",
        "             if self.target_holded >4:\n",
        "                self.model.stop_training = True\n",
        "             self.target_holded\n",
        "\n",
        "    history = model.fit(train_generator,\n",
        "        steps_per_epoch=40,\n",
        "        epochs=40,\n",
        "        validation_data=valid_generator,\n",
        "        validation_steps=20,\n",
        "        callbacks=[C3Callback()])\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_C3()\n",
        "    model.save(\"model_C3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AjxFmJepxY4"
      },
      "outputs": [],
      "source": [
        "model.save(\"model_C3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o11dFP9X1TZP",
        "outputId": "bb821db4-9c43-4998-ef1e-554a752a20a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "625/625 - 2s - loss: 0.6766 - accuracy: 0.5666 - val_loss: 0.6390 - val_accuracy: 0.6169 - 2s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "625/625 - 2s - loss: 0.5437 - accuracy: 0.7434 - val_loss: 0.4671 - val_accuracy: 0.7903 - 2s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "625/625 - 2s - loss: 0.4286 - accuracy: 0.8090 - val_loss: 0.4210 - val_accuracy: 0.8091 - 2s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "625/625 - 2s - loss: 0.3977 - accuracy: 0.8203 - val_loss: 0.4085 - val_accuracy: 0.8143 - 2s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "625/625 - 2s - loss: 0.3820 - accuracy: 0.8293 - val_loss: 0.4091 - val_accuracy: 0.8111 - 2s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "625/625 - 4s - loss: 0.3735 - accuracy: 0.8298 - val_loss: 0.4045 - val_accuracy: 0.8089 - 4s/epoch - 6ms/step\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================================================\n",
        "# PROBLEM C4\n",
        "#\n",
        "# Build and train a classifier for the sarcasm dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid.\n",
        "#\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# Dataset used in this problem is built by Rishabh Misra (https://rishabhmisra.github.io/publications).\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 75%\n",
        "# =======================================================================================================\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_C4():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sarcasm.json'\n",
        "    urllib.request.urlretrieve(data_url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Make sure you used all of these parameters or test may fail\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    padding_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    with open(\"./sarcasm.json\", 'r') as file_content:\n",
        "      data = json.load(file_content)\n",
        "\n",
        "    for texts in data:\n",
        "      sentences.append(texts['headline'])\n",
        "      labels.append(texts['is_sarcastic'])\n",
        "\n",
        "    train_sentences = sentences[0:training_size]\n",
        "    valid_sentences = sentences[training_size:]\n",
        "    train_labels = labels[0:training_size]\n",
        "    valid_labels = labels[training_size:]\n",
        "    # Fit your tokenizer with training data\n",
        "    tokenizer =  tokenizer =  Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    trainSeq = tokenizer.texts_to_sequences(train_sentences)\n",
        "    trainPad_seq = pad_sequences(trainSeq, padding=padding_type, maxlen=max_length, truncating=trunc_type)\n",
        "    validSeq = tokenizer.texts_to_sequences(valid_sentences)\n",
        "    validPad_seq = pad_sequences(validSeq, padding=padding_type, maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "    train_labels = np.array(train_labels)\n",
        "    valid_labels = np.array(valid_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        # YOUR CODE HERE. DO not change the last layer or test may fail\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(units=8, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    class C4Callback(tf.keras.callbacks.Callback):\n",
        "        acc_goal_accepted = 0\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if (logs.get('accuracy') > 0.75 and logs.get('val_accuracy') > 0.75):\n",
        "                self.acc_goal_accepted += 1\n",
        "                if self.acc_goal_accepted > 3:\n",
        "                    self.model.stop_training = True\n",
        "                self.acc_goal_accepted\n",
        "\n",
        "    model.fit(\n",
        "        trainPad_seq,\n",
        "        train_labels,\n",
        "        epochs=100,\n",
        "        validation_data=(\n",
        "            validPad_seq,\n",
        "            valid_labels),\n",
        "        verbose=2,\n",
        "        callbacks=[C4Callback()])\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_C4()\n",
        "    model.save(\"model_C4.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dup6Bf7TGAvu",
        "outputId": "ef7f0f3e-56af-4f58-8ab1-67541ed3b396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 1), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 1), dtype=tf.float64, name=None))>\n",
            "(2500,)\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 16s 470ms/step - loss: 0.0808 - mean_absolute_error: 0.3723\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 3s 254ms/step - loss: 0.0529 - mean_absolute_error: 0.2898\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 3s 249ms/step - loss: 0.0289 - mean_absolute_error: 0.2014\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0166 - mean_absolute_error: 0.1454\n",
            "mae less than 0.2, stop training!\n",
            "10/10 [==============================] - 4s 430ms/step - loss: 0.0166 - mean_absolute_error: 0.1454\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.1205\n",
            "mae less than 0.2, stop training!\n",
            "10/10 [==============================] - 5s 479ms/step - loss: 0.0117 - mean_absolute_error: 0.1205\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.1131\n",
            "mae less than 0.2, stop training!\n",
            "10/10 [==============================] - 3s 253ms/step - loss: 0.0103 - mean_absolute_error: 0.1131\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0100 - mean_absolute_error: 0.1116\n",
            "mae less than 0.2, stop training!\n",
            "10/10 [==============================] - 3s 250ms/step - loss: 0.0100 - mean_absolute_error: 0.1116\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.1116\n",
            "mae less than 0.2, stop training!\n",
            "10/10 [==============================] - 3s 256ms/step - loss: 0.0099 - mean_absolute_error: 0.1116\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.1115\n",
            "mae less than 0.2, stop training!\n",
            "10/10 [==============================] - 6s 489ms/step - loss: 0.0099 - mean_absolute_error: 0.1115\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================================\n",
        "# PROBLEM C5\n",
        "#\n",
        "# Build and train a neural network model using the Daily Min Temperature.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from https://github.com/jbrownlee/Datasets\n",
        "#\n",
        "# Desired MAE < 0.19 on the normalized dataset.\n",
        "# ============================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import urllib\n",
        "\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_C5():\n",
        "    data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
        "    urllib.request.urlretrieve(data_url, 'daily-min-temperatures.csv')\n",
        "\n",
        "    time_step = []\n",
        "    temps = []\n",
        "\n",
        "    with open('daily-min-temperatures.csv') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        step = 0\n",
        "        for row in reader:\n",
        "            temps.append(float(row[1]))  # YOUR CODE HERE)\n",
        "            time_step.append(row[0])  # YOUR CODE HERE)\n",
        "            step=step + 1\n",
        "\n",
        "    series= temps # YOUR CODE HERE\n",
        "\n",
        "    # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "    min=np.min(series)\n",
        "    max=np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time=np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time=2500\n",
        "\n",
        "    time_train= time[:split_time] # YOUR CODE HERE\n",
        "    x_train= series[:split_time] # YOUR CODE HERE\n",
        "    time_valid= time[split_time:] # YOUR CODE HERE\n",
        "    x_valid= series[split_time:] # YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size=64\n",
        "    batch_size=256\n",
        "    shuffle_buffer_size=1000\n",
        "\n",
        "    train_set=windowed_dataset(\n",
        "        x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "    print(train_set)\n",
        "    print(x_train.shape)\n",
        "\n",
        "    model=tf.keras.models.Sequential([\n",
        "        # YOUR CODE HERE.\n",
        "        tf.keras.layers.Conv1D(filters=64, kernel_size=5,\n",
        "                               strides=1, padding=\"causal\",\n",
        "                               activation=\"relu\", input_shape=[None, 1]),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8)),\n",
        "        tf.keras.layers.Dense(60, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "    class C5endEpoch(tf.keras.callbacks.Callback):\n",
        "        acc_goal_accepted = 0\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if logs[\"mean_absolute_error\"] < 0.190:\n",
        "                print('\\nmae less than 0.2, stop training!')\n",
        "                self.acc_goal_accepted += 1\n",
        "                if self.acc_goal_accepted > 5:\n",
        "                    self.model.stop_training = True\n",
        "                self.acc_goal_accepted\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9),\n",
        "                  metrics=[\"mean_absolute_error\"])\n",
        "    model.fit(train_set, epochs=100, verbose=1, callbacks=[C5endEpoch()])\n",
        "    # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_C5()\n",
        "    model.save(\"model_C5.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0QremZPWwNQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uVnlrnPn9BXD"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMX+Y2YblwnRaP2v5Ye75jg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}